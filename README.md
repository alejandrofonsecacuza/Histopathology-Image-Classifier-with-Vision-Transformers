# Histopathology Image Classifier with Vision Transformers (ViT)

Este proyecto implementa un sistema de **clasificaci√≥n autom√°tica de im√°genes histopatol√≥gicas de c√°ncer de endometrio** utilizando **Vision Transformers (ViT)**.

El modelo propuesto logra un rendimiento superior frente a arquitecturas convolucionales tradicionales y redes espec√≠ficas como **HIENet**, adem√°s de superar el desempe√±o de especialistas humanos en la misma tarea.

---

## ‚ú® Caracter√≠sticas principales

* **Vision Transformer (ViT)** preentrenado y ajustado para 4 clases de tejidos endometriales.
* **Transfer Learning & Fine-Tuning** en dataset histopatol√≥gico p√∫blico.
* **Pipeline robusto de preprocesamiento y data augmentation estratificado**.
* **Explainable AI (XAI)**: mapas de atenci√≥n para visualizar regiones diagn√≥sticamente relevantes.
* **Comparaci√≥n con estado del arte**: HIENet y expertos humanos.
* Implementaci√≥n en **PyTorch** con t√©cnicas modernas de optimizaci√≥n.

---
## üìä Dataset
El conjunto de datos utilizado est√° disponible p√∫blicamente en [Figshare](https://figshare.com/articles/dataset/A_histopathological_image_dataset_for_endometrial_disease_diagnosis/7306361).  
Contiene **3300 im√°genes histopatol√≥gicas** clasificadas en 4 categor√≠as principales:

- `NE`: Endometrio Normal  
- `EA`: Adenocarcinoma Endometrioide  
- `EP`: P√≥lipo Endometrial  
- `EH`: Hiperplasia Endometrial 

![Categor√≠as principales](docs/cells.png)

---

## üß¨ Pipeline de procesamiento de im√°genes

1. **Carga del dataset** (3300 im√°genes en 4 clases desde [Figshare](https://figshare.com/articles/dataset/A_histopathological_image_dataset_for_endometrial_disease_diagnosis/7306361)).
2. **Preprocesamiento**:

   * Redimensionamiento a 224√ó224 px.
   * Normalizaci√≥n *Min-Max scaler* a rango \[0,1].
3. **Data Augmentation estratificado**:

   * Rotaciones, flips, traslaciones, cambios de escala, ajustes de brillo y saturaci√≥n.
   * Manteniendo proporciones balanceadas entre clases.
4. **Particionamiento de datos**:

   * 70% entrenamiento ‚Äì 15% validaci√≥n ‚Äì 15% prueba.
5. **Entrenamiento con Transfer Learning**:

   * Capa de salida adaptada a 4 clases.
   * Fine-tuning completo con regularizaci√≥n y scheduler coseno.
6. **Evaluaci√≥n**:

   * M√©tricas: accuracy, precision, recall, F1-score.
   * Matriz de confusi√≥n y an√°lisis interpretativo con XAI.


![Pipeline de procesamiento de im√°genes](docs/pipeline.png)

---

## ‚öôÔ∏è Hiperpar√°metros principales

| Hiperpar√°metro     | Valor             |
| ------------------ | ----------------- |
| √âpocas             | 10                |
| Tama√±o de lote     | 32                |
| Learning Rate      | 3e-5 (0.00003)    |
| Optimizador        | AdamW             |
| Weight Decay       | 1e-4 (0.0001)     |
| Scheduler          | CosineAnnealingLR |
| Funci√≥n de p√©rdida | CrossEntropyLoss  |
| Imagen input size  | 224√ó224 px        |

---

## üìä Resultados obtenidos

El modelo **ViT** alcanz√≥ una **accuracy del 80.28%**, con un F1-score macro de **81%**, superando tanto a HIENet como a pat√≥logos humanos en la misma tarea.

### M√©tricas por clase

| Clase              | Precisi√≥n  | Sensibilidad | F1-score   |
| ------------------ | ---------- | ------------ | ---------- |
| NE                 | 84.49%     | 79.00%       | 81.65%     |
| EA                 | 89.53%     | 95.06%       | 92.21%     |
| EP                 | 66.97%     | 76.04%       | 71.21%     |
| EH                 | 79.13%     | 75.83%       | 77.44%     |
| **Promedio Macro** | **80.00%** | **81.00%**   | **81.00%** |

### Comparaci√≥n con otros m√©todos

| M√©todo / Investigadores       | Accuracy (%) |
| ----------------------------- | ------------ |
| Investigador 1                | 71.00        |
| Investigador 2                | 59.00        |
| Investigador 3                | 58.00        |
| **Promedio humanos**          | 62.67        |
| **HIENet (Sun et al., 2019)** | 76.91        |
| **ViT (este trabajo)**        | **80.28**    |

üëâ El **ViT mejora en un +3.4% sobre HIENet** y en **+17.61% respecto al promedio humano**, confirmando el poder de los mecanismos de autoatenci√≥n para capturar patrones morfol√≥gicos complejos.

---

## üìà Ejemplos de resultados

* **Matriz de confusi√≥n** del modelo ViT:

![Confusion Matrix](docs/mc.png)

## üîç Inteligencia Artificial Explicativa (XAI)

Para garantizar la interpretabilidad cl√≠nica del modelo, se integraron t√©cnicas de IA explicativa sobre el Vision Transformer.

El m√©todo utilizado fue **Attention Rollout**, complementado con t√©cnicas similares a Grad-CAM adaptadas a Transformers.

* **Mapas de atenci√≥n (XAI)** que muestran las regiones clave en la decisi√≥n:

![Attention Map](docs/heatmap.png)

---

## üìö Tecnolog√≠as

* [PyTorch](https://pytorch.org/)
* [Timm](https://github.com/huggingface/pytorch-image-models)
* [Scikit-learn](https://scikit-learn.org/)
* [Matplotlib / Seaborn](https://matplotlib.org/)

---

## üöÄ Pr√≥ximos pasos

* Ampliar dataset con im√°genes adicionales para clases desbalanceadas.
* Explorar arquitecturas h√≠bridas (CNN + Transformer).


